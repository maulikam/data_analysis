{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmjLLkYKrPO3",
        "outputId": "d7007c49-38e1-40bb-a4da-7e9a742bc11a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading second file...\n",
            "Loading first file in chunks...\n",
            "Determining column types...\n",
            "Column types for Sample 1:\n",
            "First Name: string\n",
            "Last Name: string\n",
            "Mobile Number: numeric\n",
            "Date: string\n",
            "Email: string\n",
            "Address: string\n",
            "Column types for Sample 2:\n",
            "Email: string\n",
            "Account Balance: string\n",
            "User ID: string\n",
            "Meter No: string\n",
            "Meter Reading: numeric\n",
            "Comparing columns...\n",
            "Similar columns:\n",
            "Email (Sample 1) and Email (Sample 2) - Similarity: 1.00\n"
          ]
        }
      ],
      "source": [
        "import dask.dataframe as dask_df\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.stats import pearsonr\n",
        "from typing import List, Tuple\n",
        "import multiprocessing as mp\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def determine_column_types(dataframe_chunk: pd.DataFrame) -> dict:\n",
        "    \"\"\"\n",
        "    Determine the data type of each column in the given dataframe chunk.\n",
        "\n",
        "    Args:\n",
        "    dataframe_chunk (pd.DataFrame): A chunk of the dataframe to analyze.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary mapping column names to their determined types ('string', 'numeric', or 'unknown').\n",
        "    \"\"\"\n",
        "    column_types = {}\n",
        "    for column_name in dataframe_chunk.columns:\n",
        "        if pd.api.types.is_string_dtype(dataframe_chunk[column_name]):\n",
        "            column_types[column_name] = 'string'\n",
        "        elif pd.api.types.is_numeric_dtype(dataframe_chunk[column_name]):\n",
        "            column_types[column_name] = 'numeric'\n",
        "        else:\n",
        "            column_types[column_name] = 'unknown'\n",
        "    return column_types\n",
        "\n",
        "def compare_columns(column1: pd.Series, column2: pd.Series) -> float:\n",
        "    \"\"\"\n",
        "    Compare two columns and return a similarity score.\n",
        "\n",
        "    For string columns, use cosine similarity on hashed vectors.\n",
        "    For numeric columns, use Pearson correlation coefficient.\n",
        "\n",
        "    Args:\n",
        "    column1 (pd.Series): First column to compare.\n",
        "    column2 (pd.Series): Second column to compare.\n",
        "\n",
        "    Returns:\n",
        "    float: Similarity score between 0 and 1.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if pd.api.types.is_string_dtype(column1) and pd.api.types.is_string_dtype(column2):\n",
        "            # For string columns, use cosine similarity on hashed vectors\n",
        "            vectorizer = HashingVectorizer(n_features=1000)\n",
        "            vector1 = vectorizer.fit_transform(column1.astype(str).fillna(''))\n",
        "            vector2 = vectorizer.transform(column2.astype(str).fillna(''))\n",
        "            vector1_mean = np.asarray(vector1.mean(axis=0))\n",
        "            vector2_mean = np.asarray(vector2.mean(axis=0))\n",
        "            return cosine_similarity(vector1_mean, vector2_mean)[0][0]\n",
        "        elif pd.api.types.is_numeric_dtype(column1) and pd.api.types.is_numeric_dtype(column2):\n",
        "            # For numeric columns, use Pearson correlation coefficient\n",
        "            return abs(pearsonr(column1.fillna(0), column2.fillna(0))[0])\n",
        "        else:\n",
        "            # If columns are of different types, return 0 similarity\n",
        "            return 0.0\n",
        "    except Exception as e:\n",
        "        print(f\"Error comparing columns: {str(e)}\")\n",
        "        return 0.0\n",
        "\n",
        "def process_chunk(chunk_df1: pd.DataFrame, full_df2: pd.DataFrame) -> List[Tuple[str, str, float]]:\n",
        "    \"\"\"\n",
        "    Process a chunk of the first dataframe, comparing its columns with all columns of the second dataframe.\n",
        "\n",
        "    Args:\n",
        "    chunk_df1 (pd.DataFrame): A chunk of the first dataframe.\n",
        "    full_df2 (pd.DataFrame): The complete second dataframe.\n",
        "\n",
        "    Returns:\n",
        "    List[Tuple[str, str, float]]: List of tuples containing (column1, column2, similarity_score)\n",
        "                                  for columns with similarity > 0.8.\n",
        "    \"\"\"\n",
        "    similar_columns = []\n",
        "    for column_name1 in chunk_df1.columns:\n",
        "        for column_name2 in full_df2.columns:\n",
        "            try:\n",
        "                similarity_score = compare_columns(chunk_df1[column_name1], full_df2[column_name2])\n",
        "                if similarity_score > 0.8:\n",
        "                    similar_columns.append((column_name1, column_name2, similarity_score))\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing columns {column_name1} and {column_name2}: {str(e)}\")\n",
        "    return similar_columns\n",
        "\n",
        "def main():\n",
        "    file_path1 = 'SampleData1.csv'\n",
        "    file_path2 = 'SampleData2.csv'\n",
        "\n",
        "    # Check if files exist\n",
        "    if not os.path.exists(file_path1) or not os.path.exists(file_path2):\n",
        "        print(\"Error: One or both input files do not exist.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    try:\n",
        "        # Load the second file completely into memory\n",
        "        print(\"Loading second file...\")\n",
        "        dataframe2 = dask_df.read_csv(file_path2).compute()\n",
        "\n",
        "        # Load the first file in chunks to handle large datasets\n",
        "        print(\"Loading first file in chunks...\")\n",
        "        dataframe1 = dask_df.read_csv(file_path1, blocksize=\"10MB\")\n",
        "\n",
        "        # Determine column types for both dataframes\n",
        "        print(\"Determining column types...\")\n",
        "        types_df1 = determine_column_types(dataframe1.head(1))\n",
        "        types_df2 = determine_column_types(dataframe2)\n",
        "\n",
        "        # Print column types for both dataframes\n",
        "        print(\"Column types for Sample 1:\")\n",
        "        for column_name, column_type in types_df1.items():\n",
        "            print(f\"{column_name}: {column_type}\")\n",
        "\n",
        "        print(\"Column types for Sample 2:\")\n",
        "        for column_name, column_type in types_df2.items():\n",
        "            print(f\"{column_name}: {column_type}\")\n",
        "\n",
        "        print(\"Comparing columns...\")\n",
        "\n",
        "        # Set up multiprocessing pool\n",
        "        process_pool = mp.Pool(mp.cpu_count())\n",
        "        similar_columns_list = []\n",
        "\n",
        "        # Process each chunk of the first dataframe\n",
        "        for delayed_chunk in dataframe1.to_delayed():\n",
        "            try:\n",
        "                computed_chunk = delayed_chunk.compute()\n",
        "                chunk_results = process_pool.apply_async(process_chunk, args=(computed_chunk, dataframe2))\n",
        "                similar_columns_list.extend(chunk_results.get())\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing chunk: {str(e)}\")\n",
        "\n",
        "        # Close and join the multiprocessing pool\n",
        "        process_pool.close()\n",
        "        process_pool.join()\n",
        "\n",
        "        # Sort the results by similarity score in descending order\n",
        "        similar_columns_list = sorted(set(similar_columns_list), key=lambda x: x[2], reverse=True)\n",
        "\n",
        "        # Print the results\n",
        "        print(\"Similar columns:\")\n",
        "        for col_name1, col_name2, similarity_score in similar_columns_list:\n",
        "            print(f\"{col_name1} (Sample 1) and {col_name2} (Sample 2) - Similarity: {similarity_score:.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def process_csv_file(csv_file_path):\n",
        "    \"\"\"\n",
        "    Process a CSV file, display its dimensions, and print specific columns.\n",
        "\n",
        "    Args:\n",
        "    csv_file_path (str): The path to the CSV file to be processed.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if the file exists\n",
        "        if not os.path.exists(csv_file_path):\n",
        "            raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n",
        "\n",
        "        # Attempt to read the CSV file\n",
        "        dataframe = pd.read_csv(csv_file_path)\n",
        "\n",
        "        # Get and print the dimensions of the dataframe\n",
        "        row_count, column_count = dataframe.shape\n",
        "        print(f'Number of Rows: {row_count}')\n",
        "        print(f'Number of Columns: {column_count}')\n",
        "\n",
        "        # Define the required columns\n",
        "        required_columns = ['First Name', 'Last Name', 'Mobile Number']\n",
        "\n",
        "        # Check if all required columns are present in the CSV\n",
        "        missing_columns = [col for col in required_columns if col not in dataframe.columns]\n",
        "        if missing_columns:\n",
        "            raise ValueError(f\"CSV file is missing the following required columns: {', '.join(missing_columns)}\")\n",
        "\n",
        "        # Print names and phone numbers\n",
        "        print(\"\\nNames and Phone Numbers:\")\n",
        "        for _, row in dataframe[required_columns].iterrows():\n",
        "            full_name = f\"{row['First Name']} {row['Last Name']}\"\n",
        "            phone_number = row['Mobile Number']\n",
        "            print(f\"Name: {full_name}, Phone Number: {phone_number}\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"File Error: {str(e)}\")\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"Error: The CSV file is empty.\")\n",
        "    except pd.errors.ParserError:\n",
        "        print(\"Error: Unable to parse the CSV file. Please check if it's a valid CSV.\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Value Error: {str(e)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {str(e)}\")\n",
        "\n",
        "# Main execution block\n",
        "if __name__ == \"__main__\":\n",
        "    csv_file_path = 'SampleData1.csv'\n",
        "    process_csv_file(csv_file_path)"
      ],
      "metadata": {
        "id": "cx3hr35qriYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83f2d446-760d-451a-d3b6-8a808bffc922"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Rows: 6\n",
            "Number of Columns: 6\n",
            "\n",
            "Names and Phone Numbers:\n",
            "Name: John Saint, Phone Number: 1234567890\n",
            "Name: Abraham Lincoln, Phone Number: 9876543021\n",
            "Name: Rahim Mohammed, Phone Number: 8765123490\n",
            "Name: Harry Potter, Phone Number: 4321567089\n",
            "Name: Bruce Wayne, Phone Number: 6789043215\n",
            "Name: Kara ZorEl, Phone Number: 1234509876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dtsHkpUrumvS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}